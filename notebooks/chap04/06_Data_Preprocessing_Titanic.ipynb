{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV8NRiEp65Ll"
      },
      "source": [
        "# Data Preprocessing with Pandas: The Titanic Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xLNFeTQ65Lo"
      },
      "source": [
        "The difference between data found in textbooks and data in the real world is that real-world data is rarely clean and homogeneous. The Titanic passenger dataset exemplifies this beautifully: it contains missing ages, cabins recorded inconsistently, and passenger information encoded in ways that are meaningful to humans but opaque to algorithms. Before any machine learning model can extract insights from such data, we must first *preprocess* it—cleaning, transforming, and reshaping the raw information into a form suitable for analysis.\n",
        "\n",
        "Data preprocessing can be thought of as the translation layer between human-readable records and machine-consumable features. Just as a translator must understand both languages to convey meaning accurately, effective preprocessing requires understanding both the domain (what the data represents) and the destination (what the algorithm expects). In this section, we'll explore the essential preprocessing techniques that transform messy, real-world data into clean, analysis-ready datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq4DjVEp65Lp"
      },
      "source": [
        "Let's begin by importing our standard libraries and loading the Titanic dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:48.703083Z",
          "iopub.status.busy": "2025-12-04T05:16:48.702975Z",
          "iopub.status.idle": "2025-12-04T05:16:49.006594Z",
          "shell.execute_reply": "2025-12-04T05:16:49.006116Z"
        },
        "id": "jTUAHkWk65Lp",
        "outputId": "240f3be5-02e9-451d-bcc0-9778361850d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (891, 12)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3      0            113803  53.1000  C123        S  \n",
              "4      0            373450   8.0500   NaN        S  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "titanic = pd.read_csv(url)\n",
        "print(f\"Dataset shape: {titanic.shape}\")\n",
        "titanic.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6A7YErc65Lq"
      },
      "source": [
        "Before we dive into specific preprocessing techniques, let's examine the structure of our data to understand what challenges await us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.034364Z",
          "iopub.status.busy": "2025-12-04T05:16:49.034188Z",
          "iopub.status.idle": "2025-12-04T05:16:49.039911Z",
          "shell.execute_reply": "2025-12-04T05:16:49.039449Z"
        },
        "id": "U9pNEqAi65Lq",
        "outputId": "84b41b2c-95b7-4a0a-9439-54a37823f303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ]
        }
      ],
      "source": [
        "titanic.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_xUWDC265Lr"
      },
      "source": [
        "From what we see here, the dataset contains a mix of numeric and text data, with several columns showing fewer non-null values than the total row count—a clear indication of missing data. As we'll see throughout this section, each of these issues requires a specific preprocessing strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxnVR2-s65Lr"
      },
      "source": [
        "## Converting Data Types\n",
        "\n",
        "Imagine you're organizing a library where some books have their page counts written as words (\"three hundred\") rather than numbers (300). Before you can sort by length or calculate averages, you need to convert everything to a consistent format. Data type conversion in Pandas works the same way—it ensures that values are stored in the format that enables the operations you need to perform.\n",
        "\n",
        "Type conversion can be thought of as a casting operation, similar to what you might encounter in statically-typed languages. Just as casting an integer to a float changes how the value is stored and what operations are valid, converting a Pandas column's dtype changes both its memory representation and its behavioral properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WPLu-Pj65Lr"
      },
      "source": [
        "### Explicit Type Conversion with `astype()`\n",
        "\n",
        "The most direct approach to type conversion is the `astype()` method, which is like a universal translator—you specify the target type, and Pandas attempts the conversion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.041074Z",
          "iopub.status.busy": "2025-12-04T05:16:49.040966Z",
          "iopub.status.idle": "2025-12-04T05:16:49.043966Z",
          "shell.execute_reply": "2025-12-04T05:16:49.043460Z"
        },
        "id": "yzUGNQJ865Lr",
        "outputId": "291f1e41-456c-43b2-ea77-37e1dfbedb49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dtypes:\n",
            "Pclass        int64\n",
            "Survived      int64\n",
            "Fare        float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Check the current types\n",
        "print(\"Original dtypes:\")\n",
        "print(titanic[['Pclass', 'Survived', 'Fare']].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titanic['Pclass'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titanic['Pclass'] = titanic['Pclass'].astype('category') # 타입 바꾸기\n",
        "titanic.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.get_dummies(titanic, columns=['Pclass', 'Sex']) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0PaFmfj65Ls"
      },
      "source": [
        "Notice that `Pclass` (passenger class: 1st, 2nd, or 3rd) is stored as an integer, but it's really a categorical variable. Similarly, `Survived` is a binary indicator (0 or 1) that could be stored more efficiently as a boolean. Let's convert these:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.044957Z",
          "iopub.status.busy": "2025-12-04T05:16:49.044852Z",
          "iopub.status.idle": "2025-12-04T05:16:49.048463Z",
          "shell.execute_reply": "2025-12-04T05:16:49.048106Z"
        },
        "id": "0V0WeLZ265Ls",
        "outputId": "ab7da257-3cb6-4067-eb93-49ecf519a2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted dtypes:\n",
            "Pclass      category\n",
            "Survived        bool\n",
            "Fare         float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Create a working copy to preserve original data\n",
        "df = titanic.copy()\n",
        "\n",
        "# Convert Pclass to category type (more memory-efficient for repeated values)\n",
        "df['Pclass'] = df['Pclass'].astype('category')\n",
        "\n",
        "# Convert Survived to boolean\n",
        "df['Survived'] = df['Survived'].astype(bool)\n",
        "\n",
        "print(\"Converted dtypes:\")\n",
        "print(df[['Pclass', 'Survived', 'Fare']].dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fTx3FTs65Ls"
      },
      "source": [
        "The result shows that `Pclass` is now a *category* dtype and `Survived` is a boolean. This means that categorical operations like `value_counts()` will be more efficient, and boolean indexing will be more semantically clear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3k9SxAo65Ls"
      },
      "source": [
        "### Handling Problematic Conversions with `to_numeric()`\n",
        "\n",
        "What happens when data doesn't convert cleanly? Consider a scenario where numeric data has been corrupted with text entries—this is surprisingly common in real-world datasets. The `astype()` method will raise an error in such cases, but `pd.to_numeric()` provides more graceful error handling.\n",
        "\n",
        "The `to_numeric()` function can be thought of as a forgiving parser that tries its best to extract numbers from messy input, with options for how to handle the inevitable failures:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.049670Z",
          "iopub.status.busy": "2025-12-04T05:16:49.049553Z",
          "iopub.status.idle": "2025-12-04T05:16:49.052448Z",
          "shell.execute_reply": "2025-12-04T05:16:49.052013Z"
        },
        "id": "rtKqobud65Ls",
        "outputId": "3fb90434-b088-497e-afa1-9aec0e601f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coerced result:\n",
            "0    22.0\n",
            "1    35.0\n",
            "2     NaN\n",
            "3    28.0\n",
            "4     NaN\n",
            "5    45.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Create a Series with problematic values\n",
        "messy_ages = pd.Series(['22', '35', 'unknown', '28', 'N/A', '45'])\n",
        "\n",
        "# Default behavior: raises an error\n",
        "# pd.to_numeric(messy_ages)  # This would raise ValueError\n",
        "\n",
        "# Coerce errors to NaN (recommended for data cleaning)\n",
        "clean_ages = pd.to_numeric(messy_ages, errors='coerce')\n",
        "print(\"Coerced result:\")\n",
        "print(clean_ages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h0u4NcL65Ls"
      },
      "source": [
        "Notice how the non-numeric values `'unknown'` and `'N/A'` have been converted to `NaN` (Not a Number). This is often preferable to dropping rows or raising errors, as it preserves the data structure while marking problematic values for later handling. As we'll see in the next section, Pandas provides sophisticated tools for dealing with these missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flTxxudn65Ls"
      },
      "source": [
        "### Parsing Dates with `to_datetime()`\n",
        "\n",
        "Date parsing is one of the trickiest aspects of data preprocessing because dates come in countless formats: \"2024-01-15\", \"01/15/2024\", \"January 15, 2024\", \"15-Jan-24\", and many more. The `pd.to_datetime()` function is like a polyglot date interpreter that can recognize and parse most common date formats automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.053479Z",
          "iopub.status.busy": "2025-12-04T05:16:49.053376Z",
          "iopub.status.idle": "2025-12-04T05:16:49.056836Z",
          "shell.execute_reply": "2025-12-04T05:16:49.056455Z"
        },
        "id": "fFT8E2Y265Lt",
        "outputId": "9050b7da-77df-4156-8d4d-50da42fa8719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed dates:\n",
            "0   2024-01-15\n",
            "1   2024-01-15\n",
            "2   2024-01-15\n",
            "3   2024-01-15\n",
            "dtype: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "# Various date formats that to_datetime can parse\n",
        "# In Pandas 2.x, use format='mixed' for heterogeneous date formats\n",
        "date_strings = pd.Series([\n",
        "    '2024-01-15',\n",
        "    '01/15/2024',\n",
        "    'January 15, 2024',\n",
        "    '15-Jan-2024'\n",
        "])\n",
        "\n",
        "parsed_dates = pd.to_datetime(date_strings, format='mixed')\n",
        "print(\"Parsed dates:\")\n",
        "print(parsed_dates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRsFQ-zU65Lt"
      },
      "source": [
        "The result shows all dates converted to a consistent `datetime64` format. This enables powerful datetime operations like extracting components (year, month, day), calculating time differences, and time-based filtering. Keep in mind that while `to_datetime()` is remarkably flexible, ambiguous formats (like \"01/02/03\") may require explicit format specification using the `format` parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk8ieds065Lt"
      },
      "source": [
        "Let's create a sample boarding date column for our Titanic data and extract useful features from it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.057925Z",
          "iopub.status.busy": "2025-12-04T05:16:49.057821Z",
          "iopub.status.idle": "2025-12-04T05:16:49.065535Z",
          "shell.execute_reply": "2025-12-04T05:16:49.065090Z"
        },
        "id": "9nQfqTHm65Lt",
        "outputId": "fc63133b-7037-4bdc-f540-a3b12e3ad7b7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>BoardingDate</th>\n",
              "      <th>BoardingMonth</th>\n",
              "      <th>BoardingDay</th>\n",
              "      <th>DayOfWeek</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>1912-04-08</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>Monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>1912-04-10</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>Wednesday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>1912-04-08</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>Monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>1912-04-08</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>Monday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>1912-04-10</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>Wednesday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Name BoardingDate  \\\n",
              "0                            Braund, Mr. Owen Harris   1912-04-08   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...   1912-04-10   \n",
              "2                             Heikkinen, Miss. Laina   1912-04-08   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)   1912-04-08   \n",
              "4                           Allen, Mr. William Henry   1912-04-10   \n",
              "\n",
              "   BoardingMonth  BoardingDay  DayOfWeek  \n",
              "0              4            8     Monday  \n",
              "1              4           10  Wednesday  \n",
              "2              4            8     Monday  \n",
              "3              4            8     Monday  \n",
              "4              4           10  Wednesday  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The Titanic departed on April 10, 1912\n",
        "# Let's create synthetic boarding dates for demonstration\n",
        "np.random.seed(42)\n",
        "boarding_offsets = np.random.randint(0, 3, size=len(df))  # 0-2 days before departure\n",
        "df['BoardingDate'] = pd.to_datetime('1912-04-10') - pd.to_timedelta(boarding_offsets, unit='D')\n",
        "\n",
        "# Extract datetime components\n",
        "df['BoardingMonth'] = df['BoardingDate'].dt.month\n",
        "df['BoardingDay'] = df['BoardingDate'].dt.day\n",
        "df['DayOfWeek'] = df['BoardingDate'].dt.day_name()\n",
        "\n",
        "df[['Name', 'BoardingDate', 'BoardingMonth', 'BoardingDay', 'DayOfWeek']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2OZ_XQK65Lt"
      },
      "source": [
        "From what we've seen in this section, type conversion is not merely a technical necessity but a semantic decision. Choosing the right dtype—category for discrete values, boolean for binary flags, datetime for temporal data—makes subsequent analysis both more efficient and more expressive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3aWmjhj65Lt"
      },
      "source": [
        "## Handling Missing Data\n",
        "\n",
        "Real-world data is rarely complete. Patient records may lack certain test results; survey respondents skip questions; sensors malfunction and fail to record measurements. These gaps in our data can be thought of as holes in a fabric—we can try to patch them, work around them, or in some cases, simply accept them as part of the pattern.\n",
        "\n",
        "Pandas uses the *sentinel value* approach to missing data, where special values (`NaN` for numeric data, `None` or `pd.NA` for other types) mark the absence of a value. This is like leaving a placeholder card in a recipe box where a recipe has been lost—the space is occupied, but meaningfully empty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JguEJIp365Lt"
      },
      "source": [
        "### Detecting Missing Values\n",
        "\n",
        "Before we can address missing data, we need to know where it lurks. Let's examine our Titanic dataset for missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.066617Z",
          "iopub.status.busy": "2025-12-04T05:16:49.066511Z",
          "iopub.status.idle": "2025-12-04T05:16:49.071781Z",
          "shell.execute_reply": "2025-12-04T05:16:49.071423Z"
        },
        "id": "IcP4BtaI65Lt",
        "outputId": "1691c682-a884-4671-817b-d6c58bcd8c93"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Missing Count</th>\n",
              "      <th>Missing Percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>177</td>\n",
              "      <td>19.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cabin</th>\n",
              "      <td>687</td>\n",
              "      <td>77.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Embarked</th>\n",
              "      <td>2</td>\n",
              "      <td>0.22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Missing Count  Missing Percent\n",
              "Age                 177            19.87\n",
              "Cabin               687            77.10\n",
              "Embarked              2             0.22"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count missing values in each column\n",
        "missing_counts = titanic.isnull().sum() # isna()로 수정\n",
        "missing_percent = (titanic.isnull().sum() / len(titanic)) * 100 # isna()\n",
        "\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Missing Count': missing_counts,\n",
        "    'Missing Percent': missing_percent.round(2)\n",
        "})\n",
        "\n",
        "# Show only columns with missing values\n",
        "missing_summary[missing_summary['Missing Count'] > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I614zIC065Lt"
      },
      "source": [
        "The output reveals three columns with missing data: `Age`, `Cabin`, and `Embarked`. Notice the different magnitudes—`Cabin` is missing for about 77% of passengers (likely because only upper-class passengers had assigned cabins), while `Embarked` is missing for just 2 passengers. These different patterns will require different handling strategies, as we'll explore next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REN7iI0K65Lu"
      },
      "source": [
        "### Removing Missing Data with `dropna()`\n",
        "\n",
        "The simplest approach to missing data is to remove it entirely. The `dropna()` method is like a strict bouncer at a club—any row (or column) with missing values is denied entry. While straightforward, this approach requires caution as it can significantly reduce your dataset size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.072865Z",
          "iopub.status.busy": "2025-12-04T05:16:49.072761Z",
          "iopub.status.idle": "2025-12-04T05:16:49.075827Z",
          "shell.execute_reply": "2025-12-04T05:16:49.075416Z"
        },
        "id": "qZPeVf4i65Lu",
        "outputId": "ed8c6962-4fe0-4bdd-eb90-a73082e83a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original rows: 891\n",
            "After dropping any missing: 183\n",
            "Data loss: 79.5%\n"
          ]
        }
      ],
      "source": [
        "# Reset df to original data\n",
        "df = titanic.copy()\n",
        "\n",
        "# Drop rows where ANY value is missing (aggressive)\n",
        "df_complete = df.dropna()\n",
        "print(f\"Original rows: {len(df)}\")\n",
        "print(f\"After dropping any missing: {len(df_complete)}\")\n",
        "print(f\"Data loss: {100 * (1 - len(df_complete)/len(df)):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoV0-u_S65Lu"
      },
      "source": [
        "The result shows dramatic data loss—we've discarded over 77% of our data! This is because `Cabin` is missing for most passengers. A more targeted approach is to drop only based on specific columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.076867Z",
          "iopub.status.busy": "2025-12-04T05:16:49.076763Z",
          "iopub.status.idle": "2025-12-04T05:16:49.079953Z",
          "shell.execute_reply": "2025-12-04T05:16:49.079447Z"
        },
        "id": "Mik1oaMd65Lu",
        "outputId": "5bd0b861-c7bb-4cec-c308-7fd7f3638739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original rows: 891\n",
            "After dropping Age/Embarked missing: 712\n",
            "Data loss: 20.1%\n"
          ]
        }
      ],
      "source": [
        "# Drop rows only where Age or Embarked is missing (more conservative)\n",
        "df_partial = df.dropna(subset=['Age', 'Embarked'])\n",
        "print(f\"Original rows: {len(df)}\")\n",
        "print(f\"After dropping Age/Embarked missing: {len(df_partial)}\")\n",
        "print(f\"Data loss: {100 * (1 - len(df_partial)/len(df)):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03PRfAvS65Lu"
      },
      "source": [
        "By targeting only `Age` and `Embarked`, we've preserved much more of our data. The `subset` parameter allows us to focus our missing-value handling on the columns that matter for our analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYm-jnqP65Lu"
      },
      "source": [
        "### Filling Missing Values with `fillna()`\n",
        "\n",
        "Rather than discarding incomplete records, we can often *impute* missing values—that is, fill them with reasonable estimates. The `fillna()` method can be thought of as a gap-filling tool that patches holes with specified values. The art lies in choosing appropriate fill values that don't distort your analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH3uM54K65Lu"
      },
      "source": [
        "#### Filling with Constants\n",
        "\n",
        "The simplest imputation strategy is to fill missing values with a constant. This works well when a sensible default exists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.081040Z",
          "iopub.status.busy": "2025-12-04T05:16:49.080933Z",
          "iopub.status.idle": "2025-12-04T05:16:49.084323Z",
          "shell.execute_reply": "2025-12-04T05:16:49.083789Z"
        },
        "id": "W2yC-yvZ65Lu",
        "outputId": "b1503411-eb59-46ba-e9b5-df6bd9a00c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cabin value counts after filling:\n",
            "Cabin\n",
            "Unknown        687\n",
            "G6               4\n",
            "C23 C25 C27      4\n",
            "B96 B98          4\n",
            "F2               3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Fill missing Cabin values with 'Unknown'\n",
        "df = titanic.copy()\n",
        "df['Cabin'] = df['Cabin'].fillna('Unknown')\n",
        "\n",
        "print(\"Cabin value counts after filling:\")\n",
        "print(df['Cabin'].value_counts().head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg3_fxlL65Lv"
      },
      "source": [
        "#### Filling with Statistical Measures\n",
        "\n",
        "For numeric columns like `Age`, filling with a statistical measure (mean, median, or mode) often makes more sense than an arbitrary constant. The median is typically preferred for skewed distributions as it's less sensitive to outliers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.085385Z",
          "iopub.status.busy": "2025-12-04T05:16:49.085276Z",
          "iopub.status.idle": "2025-12-04T05:16:49.088968Z",
          "shell.execute_reply": "2025-12-04T05:16:49.088510Z"
        },
        "id": "mPdUamj765Lv",
        "outputId": "cb0eb36a-e546-4545-b932-b6b1f6c74c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Age statistics (before filling):\n",
            "  Mean: 29.70\n",
            "  Median: 28.00\n",
            "  Missing: 177\n",
            "\n",
            "After filling with median (28.0):\n",
            "  Missing: 0\n"
          ]
        }
      ],
      "source": [
        "# Calculate statistics before filling\n",
        "print(f\"Age statistics (before filling):\")\n",
        "print(f\"  Mean: {df['Age'].mean():.2f}\")\n",
        "print(f\"  Median: {df['Age'].median():.2f}\")\n",
        "print(f\"  Missing: {df['Age'].isnull().sum()}\")\n",
        "\n",
        "# Fill with median (more robust to outliers)\n",
        "median_age = df['Age'].median()\n",
        "df['Age'] = df['Age'].fillna(median_age)\n",
        "\n",
        "print(f\"\\nAfter filling with median ({median_age}):\")\n",
        "print(f\"  Missing: {df['Age'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JXS5UC565Lv"
      },
      "source": [
        "#### Group-Wise Imputation\n",
        "\n",
        "A more sophisticated approach is to fill missing values based on group membership. For example, we might expect passengers of different classes or genders to have different age distributions. This technique can be thought of as context-aware imputation—using what we know about related passengers to make better guesses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.090056Z",
          "iopub.status.busy": "2025-12-04T05:16:49.089945Z",
          "iopub.status.idle": "2025-12-04T05:16:49.094562Z",
          "shell.execute_reply": "2025-12-04T05:16:49.094063Z"
        },
        "id": "tD7GZNeP65Lv",
        "outputId": "084c66bc-2bd0-411d-d0c9-dcbf4565b786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Median age by Pclass and Sex:\n",
            "Pclass  Sex   \n",
            "1       female    35.0\n",
            "        male      40.0\n",
            "2       female    28.0\n",
            "        male      30.0\n",
            "3       female    21.5\n",
            "        male      25.0\n",
            "Name: Age, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Reset and demonstrate group-wise imputation\n",
        "df = titanic.copy()\n",
        "\n",
        "# Calculate median age by Pclass and Sex\n",
        "median_by_group = df.groupby(['Pclass', 'Sex'])['Age'].transform('median')\n",
        "\n",
        "# Fill missing ages with group-specific median\n",
        "df['Age'] = df['Age'].fillna(median_by_group)\n",
        "\n",
        "# Check the group medians used\n",
        "print(\"Median age by Pclass and Sex:\")\n",
        "print(titanic.groupby(['Pclass', 'Sex'])['Age'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jij9f2Ei65Lv"
      },
      "source": [
        "Notice how the median ages vary significantly by group—first-class females tend to be older, while third-class males tend to be younger. By using group-specific medians, our imputation better preserves the underlying structure of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjqqJFrE65Ly"
      },
      "source": [
        "#### Filling with Forward/Backward Fill\n",
        "\n",
        "For time-series or ordered data, forward fill (`ffill`) and backward fill (`bfill`) propagate the last known value. These methods can be thought of as assuming that missing values should equal their temporal neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.095636Z",
          "iopub.status.busy": "2025-12-04T05:16:49.095531Z",
          "iopub.status.idle": "2025-12-04T05:16:49.098710Z",
          "shell.execute_reply": "2025-12-04T05:16:49.098154Z"
        },
        "id": "r86sJNLT65Ly",
        "outputId": "c11281c5-91ae-4b4b-e210-75fda631af07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: [ 1. nan nan  4. nan  6.]\n",
            "Forward fill: [1. 1. 1. 4. 4. 6.]\n",
            "Backward fill: [1. 4. 4. 4. 6. 6.]\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate forward and backward fill\n",
        "sample = pd.Series([1.0, np.nan, np.nan, 4.0, np.nan, 6.0])\n",
        "\n",
        "print(\"Original:\", sample.values)\n",
        "print(\"Forward fill:\", sample.ffill().values)\n",
        "print(\"Backward fill:\", sample.bfill().values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIMv6h7865Ly"
      },
      "source": [
        "In the next section, we'll explore how to encode categorical variables—a crucial step for preparing data for machine learning algorithms, which typically require numeric input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOxQR5J65Lz"
      },
      "source": [
        "## Encoding Categorical Variables\n",
        "\n",
        "Machine learning algorithms speak the language of numbers. While humans can easily understand categories like \"male\" and \"female\" or \"first class\" and \"third class\", algorithms require these concepts to be translated into numerical form. Encoding categorical variables can be thought of as creating a numeric vocabulary that captures the meaning of categorical labels.\n",
        "\n",
        "There are two main encoding strategies, each suited to different types of categorical data: *label encoding* for ordinal categories with a natural order, and *one-hot encoding* for nominal categories with no inherent ranking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1exmYLO65Lz"
      },
      "source": [
        "### Label Encoding with `map()`\n",
        "\n",
        "Label encoding assigns each category a numeric value. This approach is like numbering items in a list—straightforward and compact. The `map()` method in Pandas provides an elegant way to perform this transformation using a dictionary that defines the mapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.099814Z",
          "iopub.status.busy": "2025-12-04T05:16:49.099709Z",
          "iopub.status.idle": "2025-12-04T05:16:49.103917Z",
          "shell.execute_reply": "2025-12-04T05:16:49.103448Z"
        },
        "id": "IYvvrT4165Lz",
        "outputId": "3eb1aab8-e588-455e-eb0f-396bfbcd3b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before and after encoding:\n",
            "      Sex  Sex_encoded\n",
            "0    male            0\n",
            "1  female            1\n"
          ]
        }
      ],
      "source": [
        "df = titanic.copy()\n",
        "\n",
        "# Define the mapping for Sex\n",
        "sex_mapping = {'male': 0, 'female': 1}\n",
        "\n",
        "# Apply the mapping\n",
        "df['Sex_encoded'] = df['Sex'].map(sex_mapping)\n",
        "\n",
        "# Verify the transformation\n",
        "print(\"Before and after encoding:\")\n",
        "print(df[['Sex', 'Sex_encoded']].drop_duplicates())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Tx_NvF65Lz"
      },
      "source": [
        "Label encoding is particularly appropriate for *ordinal* categories—those with a natural order. Consider the `Embarked` column, which indicates the port of embarkation. If we wanted to encode the ports by their order along the voyage (Southampton → Cherbourg → Queenstown), label encoding would preserve this ordering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.105015Z",
          "iopub.status.busy": "2025-12-04T05:16:49.104909Z",
          "iopub.status.idle": "2025-12-04T05:16:49.109280Z",
          "shell.execute_reply": "2025-12-04T05:16:49.108814Z"
        },
        "id": "Kj-bRSGQ65Lz",
        "outputId": "869eee15-30f0-427a-9095-af3e15c6c2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embarkation port encoding:\n",
            "  Embarked  Embarked_encoded\n",
            "0        S               0.0\n",
            "1        C               1.0\n",
            "5        Q               2.0\n"
          ]
        }
      ],
      "source": [
        "# Encode Embarked by voyage order\n",
        "embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}  # Southampton, Cherbourg, Queenstown\n",
        "df['Embarked_encoded'] = df['Embarked'].map(embarked_mapping)\n",
        "\n",
        "print(\"Embarkation port encoding:\")\n",
        "print(df[['Embarked', 'Embarked_encoded']].drop_duplicates().dropna())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8niQCFcT65Lz"
      },
      "source": [
        "Notice the potential point of confusion here: label encoding implies an ordering relationship between categories. For nominal categories (like colors or country names) where no such ordering exists, one-hot encoding is usually more appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0ewDZX365Lz"
      },
      "source": [
        "### One-Hot Encoding with `pd.get_dummies()`\n",
        "\n",
        "One-hot encoding can be thought of as creating a binary flag for each category. Instead of a single column with numeric labels, we create multiple columns—one per category—where each contains 1 if the row belongs to that category and 0 otherwise. This is like having separate yes/no checkboxes for each possible answer.\n",
        "\n",
        "The `pd.get_dummies()` function automates this transformation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.110409Z",
          "iopub.status.busy": "2025-12-04T05:16:49.110301Z",
          "iopub.status.idle": "2025-12-04T05:16:49.113675Z",
          "shell.execute_reply": "2025-12-04T05:16:49.113295Z"
        },
        "id": "-kl4Opbo65Lz",
        "outputId": "52a84a98-8953-4df4-9988-480568fe1c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-hot encoded Embarked:\n",
            "   Embarked_C  Embarked_Q  Embarked_S\n",
            "0       False       False        True\n",
            "1        True       False       False\n",
            "2       False       False        True\n",
            "3       False       False        True\n",
            "4       False       False        True\n",
            "5       False        True       False\n",
            "6       False       False        True\n",
            "7       False       False        True\n",
            "8       False       False        True\n",
            "9        True       False       False\n"
          ]
        }
      ],
      "source": [
        "# One-hot encode the Embarked column\n",
        "embarked_dummies = pd.get_dummies(df['Embarked'], prefix='Embarked')\n",
        "\n",
        "print(\"One-hot encoded Embarked:\")\n",
        "print(embarked_dummies.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRrPIQ0865Lz"
      },
      "source": [
        "The result shows three new columns: `Embarked_C`, `Embarked_Q`, and `Embarked_S`. Each row has exactly one `True` (or 1) value, indicating which port that passenger boarded at. The `prefix` parameter adds a descriptive prefix to distinguish these columns from others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSmjNGe265L0"
      },
      "source": [
        "Let's apply one-hot encoding to multiple categorical columns at once and merge them back into our DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.114802Z",
          "iopub.status.busy": "2025-12-04T05:16:49.114696Z",
          "iopub.status.idle": "2025-12-04T05:16:49.121414Z",
          "shell.execute_reply": "2025-12-04T05:16:49.120937Z"
        },
        "id": "z4bCzRGl65L0",
        "outputId": "38b1359c-95ea-42c3-800c-d72808e4db42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New columns created: ['Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S\n",
              "0           0         1           0           0           1\n",
              "1           1         0           1           0           0\n",
              "2           1         0           0           0           1\n",
              "3           1         0           0           0           1\n",
              "4           0         1           0           0           1"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select columns for one-hot encoding\n",
        "df = titanic.copy()\n",
        "\n",
        "# One-hot encode Sex and Embarked, converting to integer (0/1)\n",
        "df_encoded = pd.get_dummies(df, columns=['Sex', 'Embarked'], dtype=int)\n",
        "\n",
        "# Show the new columns\n",
        "new_cols = [col for col in df_encoded.columns if 'Sex_' in col or 'Embarked_' in col]\n",
        "print(f\"New columns created: {new_cols}\")\n",
        "df_encoded[new_cols].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHX-5-eu65L0"
      },
      "source": [
        "#### Avoiding the Dummy Variable Trap\n",
        "\n",
        "When using one-hot encoded features in linear models, including all dummy columns creates perfect multicollinearity—knowing the values of all but one column perfectly predicts the remaining one. To avoid this, we can drop one category using `drop_first=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.122495Z",
          "iopub.status.busy": "2025-12-04T05:16:49.122382Z",
          "iopub.status.idle": "2025-12-04T05:16:49.128774Z",
          "shell.execute_reply": "2025-12-04T05:16:49.128331Z"
        },
        "id": "y2knzPYF65L0",
        "outputId": "3452bd8c-2b6b-4413-9d98-9f750b9a3582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns after drop_first: ['Sex_male', 'Embarked_Q', 'Embarked_S']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sex_male  Embarked_Q  Embarked_S\n",
              "0         1           0           1\n",
              "1         0           0           0\n",
              "2         0           0           1\n",
              "3         0           0           1\n",
              "4         1           0           1"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# One-hot encode with drop_first to avoid multicollinearity\n",
        "df = titanic.copy()\n",
        "df_encoded = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True, dtype=int)\n",
        "\n",
        "new_cols = [col for col in df_encoded.columns if 'Sex_' in col or 'Embarked_' in col]\n",
        "print(f\"Columns after drop_first: {new_cols}\")\n",
        "df_encoded[new_cols].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUnHOjAC65L0"
      },
      "source": [
        "Notice that now we have `Sex_male` (female is the reference category) and `Embarked_Q` and `Embarked_S` (C is the reference category). This maintains all the information while avoiding linear dependence between features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oOlUIa565L0"
      },
      "source": [
        "## Extracting Features from Strings\n",
        "\n",
        "Text data often contains valuable information locked away in unstructured formats. The passenger names in our Titanic dataset are a perfect example—embedded within each name is a title (Mr., Mrs., Miss, etc.) that reveals social status, marital status, and approximate age range. Extracting these hidden features is like mining for gold in text: the value is there, but it requires the right tools to uncover it.\n",
        "\n",
        "Pandas provides the `.str` accessor, which can be thought of as a gateway to a rich collection of string manipulation methods. Combined with regular expressions, it becomes a powerful tool for pattern-based text extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drfOwc9K65L0"
      },
      "source": [
        "### Extracting Titles with Regular Expressions\n",
        "\n",
        "Let's examine the name format in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.129934Z",
          "iopub.status.busy": "2025-12-04T05:16:49.129822Z",
          "iopub.status.idle": "2025-12-04T05:16:49.132321Z",
          "shell.execute_reply": "2025-12-04T05:16:49.131851Z"
        },
        "id": "hL-1OdKm65L0",
        "outputId": "1deb9139-890c-4085-dc2b-4a36e17ac680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample names:\n",
            "  Braund, Mr. Owen Harris\n",
            "  Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n",
            "  Heikkinen, Miss. Laina\n",
            "  Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
            "  Allen, Mr. William Henry\n"
          ]
        }
      ],
      "source": [
        "df = titanic.copy()\n",
        "print(\"Sample names:\")\n",
        "for name in df['Name'].head(5):\n",
        "    print(f\"  {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lEy0sDo65L1"
      },
      "source": [
        "We can see a consistent pattern: names follow the format \"Last, Title. First (Maiden)\". The title appears after the comma and before the period. We can extract this using the `str.extract()` method with a regular expression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.133366Z",
          "iopub.status.busy": "2025-12-04T05:16:49.133259Z",
          "iopub.status.idle": "2025-12-04T05:16:49.136804Z",
          "shell.execute_reply": "2025-12-04T05:16:49.136318Z"
        },
        "id": "01RWW2sB65L1",
        "outputId": "eb8828de-7bfc-4068-e93a-e9e5ab2a9355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title distribution:\n",
            "Title\n",
            "Mr          517\n",
            "Miss        182\n",
            "Mrs         125\n",
            "Master       40\n",
            "Dr            7\n",
            "Rev           6\n",
            "Mlle          2\n",
            "Major         2\n",
            "Col           2\n",
            "Don           1\n",
            "Mme           1\n",
            "Ms            1\n",
            "Sir           1\n",
            "Lady          1\n",
            "Capt          1\n",
            "Jonkheer      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Extract title using regex: match word characters followed by a period\n",
        "# The pattern captures text between ', ' and '.'\n",
        "df['Title'] = df['Name'].str.extract(r', ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "print(\"Title distribution:\")\n",
        "print(df['Title'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-UDsiIA65L1"
      },
      "source": [
        "The output reveals a rich variety of titles, from common ones (Mr, Miss, Mrs) to rare ones (Lady, Countess, Capt). As we'll see shortly, we can consolidate these rare titles to reduce noise in our features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqS4Lwhg65L1"
      },
      "source": [
        "### Consolidating Rare Categories\n",
        "\n",
        "Having too many categories can cause problems—rare categories have few examples to learn from and can lead to overfitting. The `replace()` method allows us to consolidate similar or rare titles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.137885Z",
          "iopub.status.busy": "2025-12-04T05:16:49.137775Z",
          "iopub.status.idle": "2025-12-04T05:16:49.141934Z",
          "shell.execute_reply": "2025-12-04T05:16:49.141468Z"
        },
        "id": "-w7KjTOL65L1",
        "outputId": "23095ba8-ef63-4bff-cc89-19fab05cfe80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consolidated title distribution:\n",
            "Title\n",
            "Mr        517\n",
            "Miss      185\n",
            "Mrs       126\n",
            "Master     40\n",
            "Rare       22\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Define mapping for rare titles\n",
        "title_mapping = {\n",
        "    'Mlle': 'Miss',\n",
        "    'Ms': 'Miss',\n",
        "    'Mme': 'Mrs',\n",
        "    'Lady': 'Rare',\n",
        "    'Countess': 'Rare',\n",
        "    'Capt': 'Rare',\n",
        "    'Col': 'Rare',\n",
        "    'Don': 'Rare',\n",
        "    'Dr': 'Rare',\n",
        "    'Major': 'Rare',\n",
        "    'Rev': 'Rare',\n",
        "    'Sir': 'Rare',\n",
        "    'Jonkheer': 'Rare',\n",
        "    'Dona': 'Rare'\n",
        "}\n",
        "\n",
        "df['Title'] = df['Title'].replace(title_mapping)\n",
        "\n",
        "print(\"Consolidated title distribution:\")\n",
        "print(df['Title'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBD1mJ-W65L1"
      },
      "source": [
        "Now we have a cleaner set of four categories: Mr, Miss, Mrs, Master, and Rare. This is much more manageable for machine learning while still preserving the key social information encoded in titles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDs3rNln65L1"
      },
      "source": [
        "### Other String Operations\n",
        "\n",
        "The `.str` accessor provides many other useful methods. Let's explore a few:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.143070Z",
          "iopub.status.busy": "2025-12-04T05:16:49.142959Z",
          "iopub.status.idle": "2025-12-04T05:16:49.148426Z",
          "shell.execute_reply": "2025-12-04T05:16:49.147939Z"
        },
        "id": "pPRqbBYF65L1",
        "outputId": "871d2fc5-8c60-4d67-9c51-c80b44771e8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                Name   LastName  NameLength  \\\n",
            "0                            Braund, Mr. Owen Harris     Braund          23   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    Cumings          51   \n",
            "2                             Heikkinen, Miss. Laina  Heikkinen          22   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)   Futrelle          44   \n",
            "4                           Allen, Mr. William Henry      Allen          24   \n",
            "\n",
            "   HasParentheses  \n",
            "0           False  \n",
            "1            True  \n",
            "2           False  \n",
            "3            True  \n",
            "4           False  \n"
          ]
        }
      ],
      "source": [
        "# Extract last name (before the comma)\n",
        "df['LastName'] = df['Name'].str.split(',').str[0]\n",
        "\n",
        "# Check name length (could indicate nobility with longer titles)\n",
        "df['NameLength'] = df['Name'].str.len()\n",
        "\n",
        "# Check if name contains certain patterns\n",
        "df['HasParentheses'] = df['Name'].str.contains(r'\\(', regex=True)\n",
        "\n",
        "print(df[['Name', 'LastName', 'NameLength', 'HasParentheses']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKlwBi5x65L2"
      },
      "source": [
        "Notice that names containing parentheses often indicate married women whose maiden names are included. This could be a useful feature for understanding family relationships aboard the ship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNE49SI165L2"
      },
      "source": [
        "### Extracting Cabin Information\n",
        "\n",
        "The `Cabin` column, despite having many missing values, contains structured information—a letter prefix indicating the deck. Let's extract this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.149551Z",
          "iopub.status.busy": "2025-12-04T05:16:49.149440Z",
          "iopub.status.idle": "2025-12-04T05:16:49.152750Z",
          "shell.execute_reply": "2025-12-04T05:16:49.152203Z"
        },
        "id": "qlo0U2X465L2",
        "outputId": "e24410ac-fa8f-459d-862b-33209b1ce984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deck distribution:\n",
            "Deck\n",
            "C    59\n",
            "B    47\n",
            "D    33\n",
            "E    32\n",
            "A    15\n",
            "F    13\n",
            "G     4\n",
            "T     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Extract deck letter from Cabin (first character)\n",
        "df['Deck'] = df['Cabin'].str.extract(r'([A-Z])', expand=False)\n",
        "\n",
        "print(\"Deck distribution:\")\n",
        "print(df['Deck'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgHV9WxE65L2"
      },
      "source": [
        "The extracted deck letters (A through G) correspond to different levels of the ship. Deck location could be correlated with survival—passengers on higher decks may have had easier access to lifeboats. From what we've seen, string extraction can uncover valuable hidden features within text columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6iaSlTq65L2"
      },
      "source": [
        "## Feature Engineering: Creating New Features\n",
        "\n",
        "Sometimes the most predictive features aren't in the original data—they need to be constructed from existing features. Feature engineering can be thought of as the creative act of synthesizing new information from raw ingredients. Just as a chef combines basic ingredients into complex flavors, a data scientist combines existing features into new ones that better capture patterns in the data.\n",
        "\n",
        "In the following sections, we'll explore several feature engineering techniques: combining features, binning continuous variables, and creating interaction terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfHjrOBi65L2"
      },
      "source": [
        "### Combining Features\n",
        "\n",
        "Individual features may not tell the whole story, but combined features can reveal important patterns. Let's create a feature for family size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.153993Z",
          "iopub.status.busy": "2025-12-04T05:16:49.153882Z",
          "iopub.status.idle": "2025-12-04T05:16:49.157273Z",
          "shell.execute_reply": "2025-12-04T05:16:49.156797Z"
        },
        "id": "_9bcGtdX65L2",
        "outputId": "59639b72-b647-43e4-bb10-8fc8f476e075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Family size distribution:\n",
            "FamilySize\n",
            "1     537\n",
            "2     161\n",
            "3     102\n",
            "4      29\n",
            "5      15\n",
            "6      22\n",
            "7      12\n",
            "8       6\n",
            "11      7\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = titanic.copy()\n",
        "\n",
        "# SibSp = number of siblings/spouses aboard\n",
        "# Parch = number of parents/children aboard\n",
        "# Add 1 for the passenger themselves\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "\n",
        "print(\"Family size distribution:\")\n",
        "print(df['FamilySize'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiOvDayu65L3"
      },
      "source": [
        "We can further categorize passengers based on whether they were traveling alone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.158565Z",
          "iopub.status.busy": "2025-12-04T05:16:49.158450Z",
          "iopub.status.idle": "2025-12-04T05:16:49.162043Z",
          "shell.execute_reply": "2025-12-04T05:16:49.161555Z"
        },
        "id": "yZj9mfFU65L3",
        "outputId": "854e1d2c-7016-4af7-8235-c73dbcf211a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Family category distribution:\n",
            "FamilyCategory\n",
            "Solo     537\n",
            "Small    292\n",
            "Large     62\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create binary feature for solo travelers\n",
        "df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
        "\n",
        "# Create family size category\n",
        "df['FamilyCategory'] = df['FamilySize'].apply(\n",
        "    lambda x: 'Solo' if x == 1 else ('Small' if x <= 4 else 'Large')\n",
        ")\n",
        "\n",
        "print(\"Family category distribution:\")\n",
        "print(df['FamilyCategory'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1RILonA65L3"
      },
      "source": [
        "### Binning Continuous Variables with `cut()` and `qcut()`\n",
        "\n",
        "Continuous variables like `Age` and `Fare` contain fine-grained information that can sometimes obscure broader patterns. Binning these variables into discrete categories can be thought of as stepping back to see the forest instead of individual trees. Pandas provides two functions for this:\n",
        "\n",
        "- `pd.cut()`: Creates bins of equal width\n",
        "- `pd.qcut()`: Creates bins of equal frequency (quantile-based)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.163104Z",
          "iopub.status.busy": "2025-12-04T05:16:49.162991Z",
          "iopub.status.idle": "2025-12-04T05:16:49.167219Z",
          "shell.execute_reply": "2025-12-04T05:16:49.166785Z"
        },
        "id": "NqNMlcyG65L3",
        "outputId": "722740bb-cbfa-4213-b578-0ff4d6c8b817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Age groups (equal width bins):\n",
            "AgeGroup_EqualWidth\n",
            "Young Adult    535\n",
            "Adult          195\n",
            "Teen            70\n",
            "Child           69\n",
            "Senior          22\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Fill missing ages first\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "\n",
        "# Bin ages into equal-width categories\n",
        "df['AgeGroup_EqualWidth'] = pd.cut(df['Age'],\n",
        "                                    bins=[0, 12, 18, 35, 60, 100],\n",
        "                                    labels=['Child', 'Teen', 'Young Adult', 'Adult', 'Senior'])\n",
        "\n",
        "print(\"Age groups (equal width bins):\")\n",
        "print(df['AgeGroup_EqualWidth'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kta1yXBf65L3"
      },
      "source": [
        "Notice that the bins have very different sizes—there are many more \"Young Adult\" and \"Adult\" passengers than \"Senior\" passengers. This is because `cut()` creates equal-width bins, not equal-frequency bins.\n",
        "\n",
        "Let's compare with `qcut()`, which creates bins with approximately equal numbers of observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.168467Z",
          "iopub.status.busy": "2025-12-04T05:16:49.168349Z",
          "iopub.status.idle": "2025-12-04T05:16:49.172543Z",
          "shell.execute_reply": "2025-12-04T05:16:49.172090Z"
        },
        "id": "sHebvBEj65L3",
        "outputId": "df98e505-ce3b-4130-c3e5-bb42bbdad5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Age groups (quantile-based bins):\n",
            "AgeGroup_Quantile\n",
            "Middle-Young    308\n",
            "Young           231\n",
            "Old             217\n",
            "Middle-Old      135\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Bin ages into equal-frequency categories\n",
        "# Note: After imputation, some quantile boundaries may coincide\n",
        "# We'll use 4 quantiles for more reliable binning\n",
        "df['AgeGroup_Quantile'] = pd.qcut(df['Age'],\n",
        "                                   q=4,\n",
        "                                   labels=['Young', 'Middle-Young', 'Middle-Old', 'Old'])\n",
        "\n",
        "print(\"Age groups (quantile-based bins):\")\n",
        "print(df['AgeGroup_Quantile'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYKPseav65L3"
      },
      "source": [
        "Now each bin contains roughly the same number of passengers. The choice between `cut()` and `qcut()` depends on your analysis goals: use `cut()` when the bin boundaries are semantically meaningful (like life stages), and `qcut()` when you want balanced groups for statistical comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar_a7nnU65L3"
      },
      "source": [
        "Let's also bin the `Fare` column, which represents ticket price and might be a proxy for socioeconomic status:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.173840Z",
          "iopub.status.busy": "2025-12-04T05:16:49.173723Z",
          "iopub.status.idle": "2025-12-04T05:16:49.178736Z",
          "shell.execute_reply": "2025-12-04T05:16:49.178334Z"
        },
        "id": "ictm0d-h65L3",
        "outputId": "b9fafd8b-eaa4-407a-823c-600e071627b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fare statistics:\n",
            "  Min: $0.00\n",
            "  Max: $512.33\n",
            "  Median: $14.45\n",
            "  Mean: $32.20\n",
            "\n",
            "Fare band distribution:\n",
            "FareBand\n",
            "Medium       224\n",
            "Low          223\n",
            "High         222\n",
            "Very High    222\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Examine fare distribution first\n",
        "print(f\"Fare statistics:\")\n",
        "print(f\"  Min: ${df['Fare'].min():.2f}\")\n",
        "print(f\"  Max: ${df['Fare'].max():.2f}\")\n",
        "print(f\"  Median: ${df['Fare'].median():.2f}\")\n",
        "print(f\"  Mean: ${df['Fare'].mean():.2f}\")\n",
        "\n",
        "# Create fare bands using quantiles\n",
        "df['FareBand'] = pd.qcut(df['Fare'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "print(\"\\nFare band distribution:\")\n",
        "print(df['FareBand'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdTUPUWp65L4"
      },
      "source": [
        "The result shows evenly distributed fare bands. Keep in mind that the \"Low\" and \"Very High\" categories span very different fare ranges due to the skewed distribution of ticket prices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEtBv28765L4"
      },
      "source": [
        "### Creating Interaction Features\n",
        "\n",
        "Sometimes the relationship between a feature and the target depends on another feature. These *interaction effects* can be captured by creating new features that combine existing ones. For example, the effect of passenger class on survival might differ by gender:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.180060Z",
          "iopub.status.busy": "2025-12-04T05:16:49.179943Z",
          "iopub.status.idle": "2025-12-04T05:16:49.184860Z",
          "shell.execute_reply": "2025-12-04T05:16:49.184304Z"
        },
        "id": "3wotOEo665L4",
        "outputId": "d4849a24-a21e-491c-ae74-df8c14934778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Survival by Pclass × Sex:\n",
            "            Survival Rate  Count\n",
            "Pclass_Sex                      \n",
            "1_female         0.968085     94\n",
            "2_female         0.921053     76\n",
            "3_female         0.500000    144\n",
            "1_male           0.368852    122\n",
            "2_male           0.157407    108\n",
            "3_male           0.135447    347\n"
          ]
        }
      ],
      "source": [
        "# Create interaction feature: Pclass × Sex\n",
        "df['Pclass_Sex'] = df['Pclass'].astype(str) + '_' + df['Sex']\n",
        "\n",
        "# Calculate survival rate by this interaction\n",
        "survival_by_interaction = df.groupby('Pclass_Sex')['Survived'].agg(['mean', 'count'])\n",
        "survival_by_interaction.columns = ['Survival Rate', 'Count']\n",
        "survival_by_interaction = survival_by_interaction.sort_values('Survival Rate', ascending=False)\n",
        "\n",
        "print(\"Survival by Pclass × Sex:\")\n",
        "print(survival_by_interaction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcBfGV5i65L4"
      },
      "source": [
        "The result reveals a striking pattern: first-class females had the highest survival rate (around 97%), while third-class males had the lowest (around 14%). This interaction between class and gender was far more predictive than either feature alone—a powerful demonstration of why feature engineering matters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tO0NML465L4"
      },
      "source": [
        "## Putting It All Together\n",
        "\n",
        "From what we've seen throughout this section, data preprocessing is not a single operation but a pipeline of transformations that prepare raw data for analysis. Let's consolidate our preprocessing steps into a complete workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.185974Z",
          "iopub.status.busy": "2025-12-04T05:16:49.185866Z",
          "iopub.status.idle": "2025-12-04T05:16:49.200909Z",
          "shell.execute_reply": "2025-12-04T05:16:49.200336Z"
        },
        "id": "GHgSVuFo65L4",
        "outputId": "e356f38f-1121-43fc-d247-3a92fd192c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (891, 12)\n",
            "Processed shape: (891, 23)\n",
            "\n",
            "Processed columns:\n",
            "['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Deck', 'FamilySize', 'IsAlone', 'Sex_male', 'Embarked_Q', 'Embarked_S', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare', 'AgeGroup_Teen', 'AgeGroup_YoungAdult', 'AgeGroup_Adult', 'AgeGroup_Senior', 'FareBand_Medium', 'FareBand_High', 'FareBand_VeryHigh']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_titanic(df):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline for Titanic data.\n",
        "    Returns a cleaned DataFrame ready for machine learning.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original\n",
        "    data = df.copy()\n",
        "\n",
        "    # 1. Handle missing values\n",
        "    # Fill Age with group median\n",
        "    data['Age'] = data.groupby(['Pclass', 'Sex'])['Age'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "    # Fill remaining with overall median (edge cases)\n",
        "    data['Age'] = data['Age'].fillna(data['Age'].median())\n",
        "\n",
        "    # Fill Embarked with mode\n",
        "    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
        "\n",
        "    # Fill Fare with median\n",
        "    data['Fare'] = data['Fare'].fillna(data['Fare'].median())\n",
        "\n",
        "    # 2. Extract features from strings\n",
        "    # Extract title\n",
        "    data['Title'] = data['Name'].str.extract(r', ([A-Za-z]+)\\.', expand=False)\n",
        "    title_mapping = {\n",
        "        'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n",
        "        'Lady': 'Rare', 'Countess': 'Rare', 'Capt': 'Rare',\n",
        "        'Col': 'Rare', 'Don': 'Rare', 'Dr': 'Rare',\n",
        "        'Major': 'Rare', 'Rev': 'Rare', 'Sir': 'Rare',\n",
        "        'Jonkheer': 'Rare', 'Dona': 'Rare'\n",
        "    }\n",
        "    data['Title'] = data['Title'].replace(title_mapping)\n",
        "\n",
        "    # Extract deck from Cabin\n",
        "    data['Deck'] = data['Cabin'].str.extract(r'([A-Z])', expand=False)\n",
        "    data['Deck'] = data['Deck'].fillna('Unknown')\n",
        "\n",
        "    # 3. Create new features\n",
        "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
        "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
        "\n",
        "    # Age bins\n",
        "    data['AgeGroup'] = pd.cut(data['Age'],\n",
        "                              bins=[0, 12, 18, 35, 60, 100],\n",
        "                              labels=['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior'])\n",
        "\n",
        "    # Fare bins\n",
        "    data['FareBand'] = pd.qcut(data['Fare'], q=4, labels=['Low', 'Medium', 'High', 'VeryHigh'])\n",
        "\n",
        "    # 4. Encode categorical variables\n",
        "    data = pd.get_dummies(data, columns=['Sex', 'Embarked', 'Title', 'AgeGroup', 'FareBand'],\n",
        "                          drop_first=True, dtype=int)\n",
        "\n",
        "    # 5. Drop unnecessary columns\n",
        "    cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
        "    data = data.drop(columns=cols_to_drop)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Apply the preprocessing pipeline\n",
        "titanic_processed = preprocess_titanic(titanic)\n",
        "\n",
        "print(f\"Original shape: {titanic.shape}\")\n",
        "print(f\"Processed shape: {titanic_processed.shape}\")\n",
        "print(f\"\\nProcessed columns:\")\n",
        "print(titanic_processed.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T05:16:49.201990Z",
          "iopub.status.busy": "2025-12-04T05:16:49.201874Z",
          "iopub.status.idle": "2025-12-04T05:16:49.210506Z",
          "shell.execute_reply": "2025-12-04T05:16:49.210017Z"
        },
        "id": "hOMTsGsA65L4",
        "outputId": "99c871e5-934e-451a-890b-29ceeb3809d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values after preprocessing:\n",
            "0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Deck</th>\n",
              "      <th>FamilySize</th>\n",
              "      <th>IsAlone</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>...</th>\n",
              "      <th>Title_Mr</th>\n",
              "      <th>Title_Mrs</th>\n",
              "      <th>Title_Rare</th>\n",
              "      <th>AgeGroup_Teen</th>\n",
              "      <th>AgeGroup_YoungAdult</th>\n",
              "      <th>AgeGroup_Adult</th>\n",
              "      <th>AgeGroup_Senior</th>\n",
              "      <th>FareBand_Medium</th>\n",
              "      <th>FareBand_High</th>\n",
              "      <th>FareBand_VeryHigh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Survived  Pclass   Age  SibSp  Parch     Fare     Deck  FamilySize  \\\n",
              "0         0       3  22.0      1      0   7.2500  Unknown           2   \n",
              "1         1       1  38.0      1      0  71.2833        C           2   \n",
              "2         1       3  26.0      0      0   7.9250  Unknown           1   \n",
              "3         1       1  35.0      1      0  53.1000        C           2   \n",
              "4         0       3  35.0      0      0   8.0500  Unknown           1   \n",
              "\n",
              "   IsAlone  Sex_male  ...  Title_Mr  Title_Mrs  Title_Rare  AgeGroup_Teen  \\\n",
              "0        0         1  ...         1          0           0              0   \n",
              "1        0         0  ...         0          1           0              0   \n",
              "2        1         0  ...         0          0           0              0   \n",
              "3        0         0  ...         0          1           0              0   \n",
              "4        1         1  ...         1          0           0              0   \n",
              "\n",
              "   AgeGroup_YoungAdult  AgeGroup_Adult  AgeGroup_Senior  FareBand_Medium  \\\n",
              "0                    1               0                0                0   \n",
              "1                    0               1                0                0   \n",
              "2                    1               0                0                1   \n",
              "3                    1               0                0                0   \n",
              "4                    1               0                0                1   \n",
              "\n",
              "   FareBand_High  FareBand_VeryHigh  \n",
              "0              0                  0  \n",
              "1              0                  1  \n",
              "2              0                  0  \n",
              "3              0                  1  \n",
              "4              0                  0  \n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify no missing values remain\n",
        "print(\"Missing values after preprocessing:\")\n",
        "print(titanic_processed.isnull().sum().sum())\n",
        "\n",
        "# Preview the processed data\n",
        "titanic_processed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBiOJXvo65L5"
      },
      "source": [
        "The result is a clean dataset with 24 features, all numeric or boolean, with no missing values. This data is now ready for machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be8WBZKA65L5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this section, we've explored the essential data preprocessing techniques that transform raw, messy data into a form suitable for analysis. As we've seen, this process involves several key steps:\n",
        "\n",
        "- **Type conversion**: Using `astype()`, `to_numeric()`, and `to_datetime()` to ensure data is stored in appropriate formats\n",
        "- **Missing value handling**: Detecting gaps with `isnull()`, removing them with `dropna()`, or filling them with `fillna()` using strategies ranging from simple constants to group-wise imputation\n",
        "- **Categorical encoding**: Translating text categories into numbers using label encoding with `map()` or one-hot encoding with `pd.get_dummies()`\n",
        "- **String feature extraction**: Mining text fields for hidden information using the `.str` accessor and regular expressions\n",
        "- **Feature engineering**: Creating new features through combination, binning with `cut()` and `qcut()`, and interaction terms\n",
        "\n",
        "These techniques form the foundation of any data science workflow. As you work with real-world datasets, you'll find that preprocessing often consumes more time than modeling—but this investment pays dividends in model performance and interpretability. In the next section, we'll build on these preprocessing skills to explore data aggregation and grouping operations."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DS_Coding",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
